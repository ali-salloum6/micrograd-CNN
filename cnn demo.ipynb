{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CNN MicroGrad demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 09:59:51.914231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 09:59:53.218055: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-02 09:59:53.218164: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-02 09:59:56.109027: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-02 09:59:56.109239: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-02 09:59:56.109260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Convolution, CNN, MLP, Neuron\n",
    "from keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.expand_dims(train_X, axis=1)\n",
    "test_X = np.expand_dims(test_X, axis=1)\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(20*20, [10])\n",
    "model = CNN([Convolution(1, 1, kernel_size=9)], mlp, is_softmax=True)\n",
    "# model = CNN([Convolution(1, 3, kernel_size=5), Convolution(3, 1, kernel_size=5)], mlp, is_softmax=True)\n",
    "# 28 - 5 + 1 = 24\n",
    "# 24 - 5 + 1 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters 4092\n"
     ]
    }
   ],
   "source": [
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(X, y, batch_size=None):\n",
    "\n",
    "    if batch_size:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    else:\n",
    "        Xb, yb = X, y\n",
    "\n",
    "    probs = model(Xb)\n",
    "    if len(probs.shape) == 1:\n",
    "        probs = np.expand_dims(probs, axis=0)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "    y_vec = np.eye(10)[yb]\n",
    "    if len(y_vec.shape) == 1:\n",
    "        y_vec = np.expand_dims(y_vec, axis=0)\n",
    "\n",
    "    losses = np.array([(yi - probi)**2 for yi, probi in zip(y_vec, probs)])\n",
    "    data_loss = np.sum(losses, axis=1) * (1.0 / len(losses))\n",
    "    all_samples_loss = np.sum(data_loss) * (1.0 / len(data_loss))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = all_samples_loss + reg_loss\n",
    "\n",
    "    if not isinstance(yb, np.ndarray):\n",
    "        yb = np.array([yb])\n",
    "\n",
    "    # accuracy\n",
    "    accuracy = [yi == y_predi for yi, y_predi in zip(yb, y_pred)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.5177250863151045, accuracy 0.0%\n",
      "step 1 loss 0.49209703232383295, accuracy 0.0%\n",
      "step 2 loss 0.5471520891032092, accuracy 0.0%\n",
      "step 3 loss 0.5264596884145148, accuracy 0.0%\n",
      "step 4 loss 0.5821447914273361, accuracy 0.0%\n",
      "step 5 loss 0.5763286852951627, accuracy 0.03125%\n",
      "step 6 loss 0.48278164656089206, accuracy 0.03125%\n",
      "step 7 loss 0.5436353818943371, accuracy 0.03125%\n",
      "step 8 loss 0.5193593455445548, accuracy 0.03125%\n",
      "step 9 loss 0.4746080962051649, accuracy 0.03125%\n",
      "step 10 loss 0.4856993136840738, accuracy 0.0625%\n",
      "step 11 loss 0.47849744948023887, accuracy 0.0625%\n",
      "step 12 loss 0.48000336078772105, accuracy 0.09375%\n",
      "step 13 loss 0.5328158623305367, accuracy 0.09375%\n",
      "step 14 loss 0.5213781123733333, accuracy 0.09375%\n",
      "step 15 loss 0.5249376561833168, accuracy 0.125%\n",
      "step 16 loss 0.5404190669995745, accuracy 0.125%\n",
      "step 17 loss 0.4979462909578527, accuracy 0.15625%\n",
      "step 18 loss 0.5075477590956398, accuracy 0.15625%\n",
      "step 19 loss 0.48031984501557606, accuracy 0.1875%\n",
      "step 20 loss 0.48528794248992346, accuracy 0.1875%\n",
      "step 21 loss 0.4935441395149611, accuracy 0.1875%\n",
      "step 22 loss 0.4266039808887292, accuracy 0.1875%\n",
      "step 23 loss 0.42065082742671983, accuracy 0.1875%\n",
      "step 24 loss 0.4896845554810907, accuracy 0.21875%\n",
      "step 25 loss 0.45185260208108774, accuracy 0.21875%\n",
      "step 26 loss 0.44015637484928627, accuracy 0.21875%\n",
      "step 27 loss 0.40876264688688857, accuracy 0.21875%\n",
      "step 28 loss 0.42056502129007645, accuracy 0.21875%\n",
      "step 29 loss 0.4591541661663712, accuracy 0.25%\n",
      "step 30 loss 0.4183329790287083, accuracy 0.25%\n",
      "step 31 loss 0.4216880081322681, accuracy 0.25%\n",
      "step 32 loss 0.379211257034712, accuracy 0.25%\n",
      "step 33 loss 0.38599881658101953, accuracy 0.25%\n",
      "step 34 loss 0.3865988072510458, accuracy 0.28125%\n",
      "step 35 loss 0.4199892572332064, accuracy 0.28125%\n",
      "step 36 loss 0.3742515966239288, accuracy 0.28125%\n",
      "step 37 loss 0.3876780580683089, accuracy 0.28125%\n",
      "step 38 loss 0.3819974034638299, accuracy 0.3125%\n",
      "step 39 loss 0.3601576517955349, accuracy 0.3125%\n",
      "step 40 loss 0.4274786627416418, accuracy 0.3125%\n",
      "step 41 loss 0.3438692340652331, accuracy 0.34375%\n",
      "step 42 loss 0.3597618873740102, accuracy 0.34375%\n",
      "step 43 loss 0.4055437762785909, accuracy 0.34375%\n",
      "step 44 loss 0.36617494183824323, accuracy 0.34375%\n",
      "step 45 loss 0.32950928257540474, accuracy 0.34375%\n",
      "step 46 loss 0.39527935563880756, accuracy 0.34375%\n",
      "step 47 loss 0.39489276989192695, accuracy 0.375%\n",
      "step 48 loss 0.34961491763236047, accuracy 0.40625%\n",
      "step 49 loss 0.3236361956528756, accuracy 0.40625%\n",
      "step 50 loss 0.3993929867189863, accuracy 0.40625%\n",
      "step 51 loss 0.34574535624139946, accuracy 0.4375%\n",
      "step 52 loss 0.3705653424289784, accuracy 0.4375%\n",
      "step 53 loss 0.38448740637363693, accuracy 0.46875%\n",
      "step 54 loss 0.37858762340910235, accuracy 0.46875%\n",
      "step 55 loss 0.31970552724886747, accuracy 0.46875%\n",
      "step 56 loss 0.3225228301710532, accuracy 0.46875%\n",
      "step 57 loss 0.35154871312047203, accuracy 0.5%\n",
      "step 58 loss 0.33162999776332375, accuracy 0.5%\n",
      "step 59 loss 0.33349261616407255, accuracy 0.5%\n",
      "step 60 loss 0.24795290234516162, accuracy 0.5625%\n",
      "step 61 loss 0.2900559193373085, accuracy 0.5625%\n",
      "step 62 loss 0.25702207194388016, accuracy 0.5625%\n",
      "step 63 loss 0.3099356488505758, accuracy 0.59375%\n",
      "step 64 loss 0.2931306126634349, accuracy 0.625%\n",
      "step 65 loss 0.27764364677171627, accuracy 0.625%\n",
      "step 66 loss 0.23396259132634503, accuracy 0.65625%\n",
      "step 67 loss 0.3088684129421685, accuracy 0.65625%\n",
      "step 68 loss 0.23039132626186107, accuracy 0.65625%\n",
      "step 69 loss 0.298984948429429, accuracy 0.6875%\n",
      "step 70 loss 0.29206674038784497, accuracy 0.6875%\n",
      "step 71 loss 0.3032197344845701, accuracy 0.6875%\n",
      "step 72 loss 0.26918202139215897, accuracy 0.71875%\n",
      "step 73 loss 0.28964314695114635, accuracy 0.71875%\n",
      "step 74 loss 0.24193209983430586, accuracy 0.71875%\n",
      "step 75 loss 0.2879141870155511, accuracy 0.71875%\n",
      "step 76 loss 0.24698574606673473, accuracy 0.71875%\n",
      "step 77 loss 0.2182411453618583, accuracy 0.75%\n",
      "step 78 loss 0.2644272635960521, accuracy 0.75%\n",
      "step 79 loss 0.22530017594618373, accuracy 0.75%\n",
      "step 80 loss 0.2517558099784444, accuracy 0.75%\n",
      "step 81 loss 0.22631459528276548, accuracy 0.75%\n",
      "step 82 loss 0.25309255992003515, accuracy 0.78125%\n",
      "step 83 loss 0.1946279320889162, accuracy 0.78125%\n",
      "step 84 loss 0.23383334230839287, accuracy 0.78125%\n",
      "step 85 loss 0.20603659489318743, accuracy 0.78125%\n",
      "step 86 loss 0.18734709347292655, accuracy 0.8125%\n",
      "step 87 loss 0.2132632898798119, accuracy 0.8125%\n",
      "step 88 loss 0.21951153698162607, accuracy 0.8125%\n",
      "step 89 loss 0.14465800611103555, accuracy 0.8125%\n",
      "step 90 loss 0.1691760492842923, accuracy 0.8125%\n",
      "step 91 loss 0.20103168506666108, accuracy 0.8125%\n",
      "step 92 loss 0.13407066374647006, accuracy 0.8125%\n",
      "step 93 loss 0.14891874223423252, accuracy 0.8125%\n",
      "step 94 loss 0.20589929918099414, accuracy 0.84375%\n",
      "step 95 loss 0.1621193167985612, accuracy 0.84375%\n",
      "step 96 loss 0.13824359950822276, accuracy 0.84375%\n",
      "step 97 loss 0.10574847635674331, accuracy 0.84375%\n",
      "step 98 loss 0.14065568209664733, accuracy 0.84375%\n",
      "step 99 loss 0.17965352936488682, accuracy 0.84375%\n"
     ]
    }
   ],
   "source": [
    "# optimization\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss(train_X, train_y, batch_size=32)\n",
    "\n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
